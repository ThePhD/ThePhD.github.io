# Speedy


### More features/speed than any other solution

Better on both memory and speed than every other kind of tool, no matter the kind of data (embedding large neural networks and targeting embedded compilers, etc.).

Link: [Data](https://github.com/ThePhD/embed#speed-results)


### Speed/Memory Matters

> gcc wants 12+ GB to build this (it's just that one 60 MB array). Using `uint64_t` instead of `uint8_t` significantly reduces gcc memory usage, but for some reason compile time explodes. - Link: [Source](https://twitter.com/oe1cxw/status/1008361214018244608)

> Then again, it would probably be wiser to put that energy in `std::embed`, which does beat all the optimizations presented here by orders of magnitude! - Link: [Source](https://cor3ntin.github.io/posts/arrays/)


### Cross-platform Matters

> Apparently this (link) is a thing...  
> \> _fatal error C1091: compiler limit: string exceeds 65535 bytes in length_  
> So maybe another time... â€“ Link: [Source](http://lists.llvm.org/pipermail/llvm-dev/2020-January/138310.html)

> `// generated content broken into pieces because MSVC is in the 1990s.` - Link: [Source](https://github.com/libnonius/nonius/blob/devel/include/nonius/reporters/html_reporter.h++#L42)


### Implementer Time Matters

This is not just Quality of Implementation.

- GCC bug has been open for 16 years about this
- LLVM bugs indicate no progress will be made (someone tried to)
- Newly developed compilers (Circle) still take exponential compiler size explosion (tokenization)
